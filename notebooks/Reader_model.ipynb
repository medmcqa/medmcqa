{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W0Qa3hwE4jZv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Bu3iS1I_4jZ0"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MedMCQADataset(Dataset):\n",
        "\n",
        "  def __init__(self,\n",
        "               csv_path):\n",
        "#     self.dataset = dataset['train'] if training == True else dataset['test']\n",
        "    self.dataset = pd.read_csv(csv_path)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    context = self.dataset.loc[idx,'exp']\n",
        "    question = self.dataset.loc[idx,'question']\n",
        "    options = self.dataset.loc[idx,['opa', 'opb', 'opc', 'opd']].values\n",
        "    label = self.dataset.loc[idx,'cop'] - 1\n",
        "    return (context,question,options,label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fmAQNP6A4jZ1"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.core.step_result import TrainResult,EvalResult\n",
        "from pytorch_lightning import Trainer\n",
        "from torch.utils.data import SequentialSampler,RandomSampler\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "from torch.utils.data import DataLoader,RandomSampler\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer,AutoModel\n",
        "import functools\n",
        "\n",
        "\n",
        "\n",
        "class MedMCQAModel(pl.LightningModule):\n",
        "  def __init__(self,\n",
        "               model_name_or_path,\n",
        "               args):\n",
        "    \n",
        "    super().__init__()\n",
        "    self.init_encoder_model(model_name_or_path)\n",
        "    self.args = args\n",
        "    self.batch_size = self.args['batch_size']\n",
        "    self.dropout = nn.Dropout(self.args['hidden_dropout_prob'])\n",
        "    self.linear = nn.Linear(in_features=self.args['hidden_size'],out_features=1)\n",
        "    self.ce_loss = nn.CrossEntropyLoss()\n",
        "    self.save_hyperparameters()\n",
        "    \n",
        "  \n",
        "  def init_encoder_model(self,model_name_or_path):\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "    self.model = AutoModel.from_pretrained(model_name_or_path)\n",
        " \n",
        "  def prepare_dataset(self,train_dataset,val_dataset,test_dataset=None):\n",
        "    \"\"\"\n",
        "    helper to set the train and val dataset. Doing it during class initialization\n",
        "    causes issues while loading checkpoint as the dataset class needs to be \n",
        "    present for the weights to be loaded.\n",
        "    \"\"\"\n",
        "    self.train_dataset = train_dataset\n",
        "    self.val_dataset = val_dataset\n",
        "    if test_dataset != None:\n",
        "        self.test_dataset = test_dataset\n",
        "    else:\n",
        "        self.test_dataset = val_dataset\n",
        "  \n",
        "  def forward(self,input_ids,attention_mask,token_type_ids):\n",
        "    outputs = self.model(input_ids=input_ids,\n",
        "                       attention_mask=attention_mask,\n",
        "                       token_type_ids=token_type_ids)\n",
        "    \n",
        "    pooled_output = outputs[1]\n",
        "    pooled_output = self.dropout(pooled_output)\n",
        "    logits = self.linear(pooled_output)\n",
        "    reshaped_logits = logits.view(-1,self.args['num_choices'])\n",
        "    return reshaped_logits\n",
        "  \n",
        "  def training_step(self,batch,batch_idx):\n",
        "    inputs,labels = batch\n",
        "    logits = self(**inputs)\n",
        "    loss = self.ce_loss(logits,labels)\n",
        "    result = TrainResult(loss)\n",
        "    result.log('train_loss', loss, on_epoch=True)\n",
        "    return result\n",
        "  \n",
        "  def test_step(self, batch, batch_idx):\n",
        "    inputs,labels = batch\n",
        "    logits = self(**inputs)\n",
        "    loss = self.ce_loss(logits,labels)\n",
        "    result = EvalResult(loss)\n",
        "    result.log('test_loss', loss, on_epoch=True)\n",
        "    result.log('logits',logits,on_epoch=True)\n",
        "    result.log('labels',labels,on_epoch=True)\n",
        "    self.log('test_loss', loss)\n",
        "    return result\n",
        " \n",
        "  def test_epoch_end(self, outputs):\n",
        "    avg_loss = outputs['test_loss'].mean()\n",
        "    predictions = torch.argmax(outputs['logits'],axis=-1)\n",
        "    labels = outputs['labels']\n",
        "    self.test_predictions = labels\n",
        "    correct_predictions = torch.sum(predictions==labels)\n",
        "    accuracy = correct_predictions.cpu().detach().numpy()/predictions.size()[0]\n",
        "    result = EvalResult(checkpoint_on=avg_loss,early_stop_on=avg_loss)\n",
        "    result.log_dict({\"test_loss\":avg_loss,\"test_acc\":accuracy},prog_bar=True,on_epoch=True)\n",
        "    self.log('avg_test_loss', avg_loss)\n",
        "    self.log('avg_test_acc', accuracy)\n",
        "    return result\n",
        "  \n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    inputs,labels = batch\n",
        "    logits = self(**inputs)\n",
        "    loss = self.ce_loss(logits,labels)\n",
        "    result = EvalResult(loss)\n",
        "    result.log('val_loss', loss, on_epoch=True)\n",
        "    result.log('logits',logits,on_epoch=True)\n",
        "    result.log('labels',labels,on_epoch=True)\n",
        "    self.log('val_loss', loss)\n",
        "    return result\n",
        "\n",
        "  def validation_epoch_end(self, outputs):\n",
        "        avg_loss = outputs['val_loss'].mean()\n",
        "        predictions = torch.argmax(outputs['logits'],axis=-1)\n",
        "        labels = outputs['labels']\n",
        "        correct_predictions = torch.sum(predictions==labels)\n",
        "        accuracy = correct_predictions.cpu().detach().numpy()/predictions.size()[0]\n",
        "        result = EvalResult(checkpoint_on=avg_loss,early_stop_on=avg_loss)\n",
        "        result.log_dict({\"val_loss\":avg_loss,\"val_acc\":accuracy},prog_bar=True,on_epoch=True)\n",
        "        self.log('avg_val_loss', avg_loss)\n",
        "        self.log('avg_val_acc', accuracy)\n",
        "        return result\n",
        "        \n",
        "  def configure_optimizers(self):\n",
        "    optimizer = AdamW(self.parameters(),lr=self.args['learning_rate'],eps=1e-8)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=100,\n",
        "        num_training_steps=(self.args['num_epochs'] + 1) * math.ceil(len(self.train_dataset) / self.args['batch_size']),\n",
        "    )\n",
        "    return [optimizer],[scheduler]\n",
        "  \n",
        "  def process_batch(self,batch,tokenizer,max_len=32):\n",
        "    expanded_batch = []\n",
        "    labels = []\n",
        "    for context,question,options,label in batch:\n",
        "        question_option_pairs = [question+' '+option for option in options]\n",
        "        contexts = [context]*len(options)\n",
        "        labels.append(label)\n",
        "        expanded_batch.extend(zip(contexts,question_option_pairs))\n",
        "    tokenized_batch = tokenizer.batch_encode_plus(expanded_batch,truncation=True,padding=\"max_length\",max_length=max_len,return_tensors=\"pt\")\n",
        "    return tokenized_batch,torch.tensor(labels)\n",
        "  \n",
        "  def train_dataloader(self):\n",
        "    train_sampler = RandomSampler(self.train_dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "      self.process_batch,\n",
        "      tokenizer=self.tokenizer,\n",
        "      max_len=self.args['max_len']\n",
        "      )\n",
        "    train_dataloader = DataLoader(self.train_dataset,\n",
        "                                batch_size=self.batch_size,\n",
        "                                sampler=train_sampler,\n",
        "                                collate_fn=model_collate_fn)\n",
        "    return train_dataloader\n",
        "  \n",
        "  def val_dataloader(self):\n",
        "    eval_sampler = SequentialSampler(self.val_dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "      self.process_batch,\n",
        "      tokenizer=self.tokenizer,\n",
        "      max_len=self.args['max_len']\n",
        "      )\n",
        "    val_dataloader = DataLoader(self.val_dataset,\n",
        "                                batch_size=self.batch_size,\n",
        "                                sampler=eval_sampler,\n",
        "                                collate_fn=model_collate_fn)\n",
        "    return val_dataloader\n",
        "  \n",
        "  def test_dataloader(self):\n",
        "    eval_sampler = SequentialSampler(self.test_dataset)\n",
        "    model_collate_fn = functools.partial(\n",
        "      self.process_batch,\n",
        "      tokenizer=self.tokenizer,\n",
        "      max_len=self.args['max_len']\n",
        "      )\n",
        "    test_dataloader = DataLoader(self.test_dataset,\n",
        "                                batch_size=self.batch_size,\n",
        "                                sampler=eval_sampler,\n",
        "                                collate_fn=model_collate_fn)\n",
        "    return test_dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cvkFkwW4jZ7"
      },
      "outputs": [],
      "source": [
        "!export WANDB_API_KEY='your_api_key'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSnLYsbI4jZ8"
      },
      "outputs": [],
      "source": [
        "class Arguments():\n",
        "    def __init__(self,\n",
        "                 pretrained_model_name='bert-base-uncased',\n",
        "                 train_csv=None,\n",
        "                 test_csv=None,\n",
        "                 dev_csv=None):\n",
        "        self.batch_size = 64\n",
        "        self.max_len = 128\n",
        "        self.checkpoint_batch_size = 32\n",
        "        self.print_freq = 100\n",
        "        self.pretrained_model_name = pretrained_model_name\n",
        "        self.model_save_name = \"retriBertStyle\"\n",
        "        self.learning_rate = 2e-4\n",
        "        self.hidden_dropout_prob=0.4\n",
        "        self.hidden_size=768\n",
        "        self.num_epochs = 1\n",
        "        self.num_choices = 4\n",
        "        self.train_csv = train_csv\n",
        "        self.test_csv = test_csv\n",
        "        self.dev_csv = dev_csv\n",
        "\n",
        "args = Arguments(train_csv=\"/home/admin/medmcqa/train.csv\",\n",
        "                 test_csv=\"/home/admin/medmcqa/test.csv\",\n",
        "                 dev_csv=\"/home/admin/medmcqa/dev.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rbbFlSeB4jZ9"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "\n",
        "\n",
        "MODELS_FOLDER = \"../models\"\n",
        "EXPERIMENT_NAME = \"medmcqa-test\" \n",
        "EXPERIMENT_FOLDER = os.path.join(MODELS_FOLDER,EXPERIMENT_NAME)\n",
        "\n",
        "wb = WandbLogger(project=\"medmcqa-bert\",name=EXPERIMENT_NAME,version=\"1\")\n",
        "csv_log = CSVLogger(MODELS_FOLDER, name=EXPERIMENT_NAME, version=None)\n",
        "\n",
        "os.makedirs(EXPERIMENT_FOLDER,exist_ok=True)\n",
        "\n",
        "train_dataset = MedMCQADataset(args.dev_csv)\n",
        "test_dataset = MedMCQADataset(args.test_csv)\n",
        "val_dataset = MedMCQADataset(args.dev_csv)\n",
        "\n",
        "\n",
        "qaModel = MedMCQAModel(model_name_or_path=args.pretrained_model_name,\n",
        "                      args=args.__dict__)\n",
        "\n",
        "qaModel.prepare_dataset(train_dataset=val_dataset,test_dataset=test_dataset,val_dataset=val_dataset)\n",
        "\n",
        "pl.seed_everything(42)\n",
        "\n",
        "es = pl.callbacks.EarlyStopping(\n",
        "   monitor='val_loss',\n",
        "   min_delta=0.00,\n",
        "   patience=2,\n",
        "   verbose=True,\n",
        "   mode='min'\n",
        ")\n",
        "\n",
        "experiment_string = EXPERIMENT_NAME+'-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}'\n",
        "\n",
        "checkpointCallback = pl.callbacks.ModelCheckpoint(monitor='val_loss',\n",
        "                                                 filepath=os.path.join(EXPERIMENT_FOLDER,experiment_string),\n",
        "                                                 save_top_k=1,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 mode='min')\n",
        "\n",
        "trainer = Trainer(gpus=-1,\n",
        "                  logger=[wb,csv_log],\n",
        "                  callbacks= [es,checkpointCallback],\n",
        "                  max_epochs=args.num_epochs)\n",
        "\n",
        "trainer.fit(qaModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGtIf4Xz4jZ-"
      },
      "outputs": [],
      "source": [
        "?Trainer.get_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggt6-6894jZ_",
        "outputId": "77a247ac-36c8-4a90-cdff-65afaccd31fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4\n"
          ]
        }
      ],
      "source": [
        "inference_qa_model = MedMCQAModel.load_from_checkpoint(\"../models/medmcqa.ckpt\",\n",
        "                                   model_name_or_path=args.pretrained_model_name,\n",
        "                                   args=args,\n",
        "                                   batch_size=args.batch_size,\n",
        "                                   train_dataset=train_dataset,                     \n",
        "                                   val_dataset=test_dataset)\n",
        "inference_qa_model = inference_qa_model.to(\"cuda\")\n",
        "inference_qa_model = inference_qa_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PKJj0wXW4jaA"
      },
      "outputs": [],
      "source": [
        "trainer.test()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lA_t2NIz4jaB"
      },
      "outputs": [],
      "source": [
        "test_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKET37Ej4jaC"
      },
      "outputs": [],
      "source": [
        "\n",
        "inference_set = [{\n",
        "    'article':\"The 1983 Cricket World Cup (officially the Prudential Cup '83) was the 3rd edition of the Cricket World Cup tournament. It was held from 9 to 25 June 1983 in England and Wales and was won by India.\",\n",
        "    'question': 'How many countries participated in 1983 world cup ?',\n",
        "    'options':['8','1','2','5'],\n",
        "    'answer':'A'\n",
        "},{\n",
        "    'article':\"The 1983 Cricket World Cup (officially the Prudential Cup '83) was the 3rd edition of the Cricket World Cup tournament. It was held from 9 to 25 June 1983 in England and Wales and was won by India. Eight countries participated in the event\",\n",
        "    'question': 'How many countries participated in 1983 world cup ?',\n",
        "    'options':['8','1','2','5'],\n",
        "    'answer':'A'\n",
        "},{\n",
        "    'article':\"Kapil Dev once again rose to the occasion as he caught West Indies Skipper Clive Lloyd off Roger Binny reeling West Indies at 66/5. Soon Faoud Bacchus lost his wicket to Sandhu. Wicket keeper Jeff Dujon and all rounder Malcolm Marshall tried a rescue act with a 43 runs partnership\",\n",
        "    'question': 'How many countries participated in 1983 world cup ?',\n",
        "    'options':['8','1','2','5'],\n",
        "    'answer':'A'\n",
        "}]\n",
        "# for i in range(10):\n",
        "#     inference_set.append(dataset['test'][i])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4gSLE8V4jaC"
      },
      "outputs": [],
      "source": [
        "inference_dataset = MedMCQADataset(inference_set)\n",
        "eval_sampler = SequentialSampler(inference_dataset)\n",
        "\n",
        "model_collate_fn = functools.partial(\n",
        "  inference_qa_model.process_batch,\n",
        "  tokenizer=inference_qa_model.tokenizer,\n",
        "  max_len=inference_qa_model.args.max_len\n",
        "  )\n",
        "\n",
        "inference_dataloader = DataLoader(inference_dataset,\n",
        "                            batch_size=32,\n",
        "                            sampler=eval_sampler,\n",
        "                            collate_fn=model_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwlL4L494jaD",
        "outputId": "a27e7e8d-d4ea-4a05-fe35-f4b7933d4b4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-3.6043, -3.9778, -2.3475, -0.8237],\n",
              "        [ 0.0462, -6.1841, -2.2088, -2.4919],\n",
              "        [-0.8242, -3.9122, -0.7614, -2.0710]], grad_fn=<ViewBackward>)"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.argmax(qaModel(**next(iter(inference_dataloader))[0]),axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94Ym6Bov4jaD",
        "outputId": "94f2fb61-02f6-468f-8ee7-9e8d2670081a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0]"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(map(lambda x:ord(x['answer'])-ord('A'),inference_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZwBdYCV4jaE"
      },
      "source": [
        "### Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mIA8GhU4jaF",
        "outputId": "067b3cc6-29bb-4234-8aa9-8ef42a0c1299"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration psgs_w100.nq.compressed\n",
            "Reusing dataset wiki_dpr (./dataset_cache/wiki_dpr/psgs_w100.nq.compressed/0.0.0/14b973bf2a456087ff69c0fd34526684eed22e48e0dfce4338f9a22b965ce7c2)\n"
          ]
        }
      ],
      "source": [
        "wiki_dataset = load_dataset(\"wiki_dpr\",cache_dir=\"./dataset_cache/\",data_dir=\"./dataset_cache/\",with_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsXfOBRU4jaG"
      },
      "outputs": [],
      "source": [
        "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
        "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv9jB-Jw4jaG"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZBaPaDk4jaH"
      },
      "outputs": [],
      "source": [
        "def retrieve_topk(question,k=20):\n",
        "    question_embedding = q_encoder(**q_tokenizer(question, return_tensors=\"pt\"))[0][0].detach().numpy()\n",
        "    scores, retrieved_examples = wiki_dataset['train'].get_nearest_examples('embeddings', question_embedding, k=k)\n",
        "    return retrieved_examples['text'],scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY4G4GTr4jaH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ysix7KsJ4jaI"
      },
      "outputs": [],
      "source": [
        "def prepare_question_dict(question,options,contexts):\n",
        "    \n",
        "    def form_question(article,question_template):\n",
        "        qt = question_template.copy()\n",
        "        qt['article'] = article\n",
        "        return qt\n",
        "\n",
        "    question_template = {\n",
        "        'question': question,\n",
        "        'options': options,\n",
        "        'answer':'A'\n",
        "    }\n",
        "\n",
        "    question_set = list(map(lambda article: form_question(article,question_template),contexts))\n",
        "    \n",
        "    return question_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQiN9UXo4jaI"
      },
      "outputs": [],
      "source": [
        "def get_answer(model,question_set):\n",
        "    inference_dataset = MedMCQADataset(question_set)\n",
        "    eval_sampler = SequentialSampler(inference_dataset)\n",
        "\n",
        "    model_collate_fn = functools.partial(\n",
        "      model.process_batch,\n",
        "      tokenizer=model.tokenizer,\n",
        "      max_len=model.args.max_len\n",
        "      )\n",
        "    \n",
        "    inference_dataloader = DataLoader(inference_dataset,\n",
        "                                batch_size=32,\n",
        "                                sampler=eval_sampler,\n",
        "                                collate_fn=model_collate_fn)\n",
        "    model(**next(iter(inference_dataloader))[0])\n",
        "    top_option = torch.argmax(torch.flatten(model(**next(iter(inference_dataloader))[0]))).item()\n",
        "    top_choice = top_option%4\n",
        "    top_sentence_idx = top_option//4\n",
        "    return top_choice,top_sentence_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQeJULA04jaJ",
        "outputId": "76246b95-a46e-4107-fa31-2bd955a9f72f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------\n",
            "Question : Acetylcholine is not used commersially because\n",
            "Predicted Answer : Rapidly destroyed in the body\n",
            "-------------------------\n",
            "Top Context : and diazepam would be an even safer anesthetic consideration, but etomidate is not commonly carried by general veterinary practitioners due to its cost. Fluid therapy is equally essential for correcting derangements. Commonly, a fluid low in potassium, such as 0.9% NaCl, is selected. If 0.9% NaCl is not available, any other crystalloid fluid is realistic even if it contains some level of potassium. Insulin is sometimes used intravenously to temporarily reduce high potassium levels. Calcium gluconate can also be used to protect the myocardium (heart muscle) from the negative effects of hyperkalemia. Rarely, an urethral obstruction cannot not be removed\n",
            "-------------------------\n",
            "Question : Tolterodine used in overactive bladder acts by which receptor:\n",
            "Predicted Answer : M3\n",
            "-------------------------\n",
            "Top Context : Allatostatin Allatostatins are neuropeptide hormones in insects and crustacea. They have a twofold function: they both inhibit the generation of juvenile hormone and reduce their food intake. They are therefore putative targets for insecticide research. Three biochemically distinct types of Allatostatin have been described: A, B and C. Although originally identified in different insects, all three types are found in the fruitfly, \"Drosophila\". These types of Allatostatin are not normally found in the same neurons, and so probably have different roles. Allatostatin is found in the cells in a small neuronal cluster, the frontal ganglion. It is also present in\n"
          ]
        }
      ],
      "source": [
        "# questions = ['Which year did india got its independence ?',\n",
        "#              'Who wrote the novel Evening Class ?']\n",
        "# options_list = [['1947','1952','1974','1930'],\n",
        "#                 ['Maeve Binchy','Orwell','Lone Scherfig','shelly']]\n",
        "\n",
        "\n",
        "questions = ['Acetylcholine is not used commersially because', 'Tolterodine used in overactive bladder acts by which receptor:']\n",
        "options_list = [['Long duration of action','Costly','Rapidly destroyed in the body','Crosses blood brain barrier'],['Ml', 'M2','M3','M4']]\n",
        "\n",
        "\n",
        "for question,options in zip(questions,options_list):\n",
        "    relevant_contexts,retrieval_scores = retrieve_topk(question,k=20)\n",
        "    question_dict = prepare_question_dict(question,options,relevant_contexts)\n",
        "    top_choice,top_sentence_idx = get_answer(inference_qa_model,question_dict)\n",
        "    print(f'-------------------------')\n",
        "    print(f'Question : {question}')\n",
        "    print(f'Predicted Answer : {question_dict[top_sentence_idx][\"options\"][top_choice]}')\n",
        "    print(f'-------------------------')\n",
        "    print(f'Top Context : {question_dict[top_sentence_idx][\"article\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIutaDV74jaK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bert-race",
      "language": "python",
      "name": "bert-race"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Reader_model.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}